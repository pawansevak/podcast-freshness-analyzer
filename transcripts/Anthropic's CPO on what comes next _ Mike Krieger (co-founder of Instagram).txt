---
podcast: Lenny's Podcast
episode: Anthropic's CPO on what comes next
guest: Mike Krieger (co-founder of Instagram)
category: learn_from_legends
---

00:00:00 - 00:01:31
90% of your code roughly is written by AI. Now, the team that works in the most futuristic way is the cloud code team. They're using cloud code to build cloud code in a very self-improving kind of way. We really rapidly became bottlenecked on other things like our merge cube. We had to completely rearchitect it because so much more code was being written and so many more pull requests were being submitted. Over half of our pull requests are cloud code generated. Probably at this point, it's probably over 70% that it just completely blew out the expectations a bit. You guys are at the edge of where things are heading. I had the very bizarre experience of I had two tabs open. It was AI 2027 and my product strategy and it was this like moment where I'm like wait am I the character in the story? It feels like Chad GBT is just winning in consumer mind share. How does that inform the way you think about product strategy and mission? I think there's room for several generationally


00:00:46 - 00:02:27
important companies to be built in AI right now. How do we figure out what we want to be when we grow up versus like what we currently aren't or wish that we were or like see other players in the space being? What's something that you've changed your mind about? what AI is capable of and where AI is heading. I had this notion coming in like yes these models are great but are they able to have an independent opinion and it's actually really flipped for me only in the last month. Today my guest is Mike Kger. Mike is chief product officer at Anthropic the company behind Claude. He's also the co-founder of Instagram. He's one of my most favorite product builders and thinkers. He's also now leading product at one of the most important companies in the world and I'm so thrilled to have had a chance to chat with him on the podcast. We chat about what he's changed his mind about most in terms of AI capabilities in the years since he joined Anthropic, how product


00:01:37 - 00:03:27
development changes and where bottlenecks emerge when 90% of your code is written by AI, which is now true at anthropic. Also, his thoughts on Open AI versus anthropic, the future of MCP, why he shut down Artifact, his last startup, and how he feels about it. also what skills he's encouraging his kids to develop with the rise of AI. And we close the podcast on a very heartwarming message that Claude wanted me to share with Mike. A big thank you to my newsletter Slack community for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. Also, if you become an annual subscriber of my newsletter, you get a year free of a bunch of incredible products, including Linear, Superhum, Notion, Perplexity, and Granola. Check it out at lennisnewsletter.com and click bundle. With that, I bring you Mike Creger. This episode is brought to you by Product Board, the leading product management platform for the enterprise. For over 10


00:02:32 - 00:04:31
years, Product Board has helped customer centric organizations like Zoom, Salesforce, and Autodesk build the right products faster. And as an end-to-end platform, Product Board seamlessly supports all stages of the product development life cycle. From gathering customer insights to planning a roadmap to aligning stakeholders to earning customer buyin, all with a single source of truth. And now product leaders can get even more visibility into customer needs with Product Board Pulse, a new voice of customer solution. Built-in intelligence helps you analyze trends across all of your feedback. And then dive deeper by asking AI your follow-up questions. See how Product Board can help your team deliver higher impact products that solve real customer needs and advance your business goals. For a special offer and free 15-day trial, visit productboard.com/lenny. That's productboard.com/leny. Last year, 1.3% of the global GDP flowed through Stripe. That's over $1.4 trillion. And driving


00:03:35 - 00:05:33
that huge number are the millions of businesses growing more rapidly with Stripe. For industry leaders like Forbes, Atlassian, OpenAI, and Toyota, Stripe isn't just financial software. It's a powerful partner that simplifies how they move money, making it as seamless and borderless as the internet itself. For example, Herz boosted its online payment authorization rates by 4% after migrating to Stripe. And imagine seeing a 23% lift in revenue like Forbes did just 6 months after switching to Stripe for subscription management. Stripe has been leveraging AI for the last decade to make its product better at growing revenue for all businesses. From smarter checkouts to fraud prevention and beyond. Join the ranks of over half of the Fortune 100 companies that trust Stripe to drive change. Learn more at stripe.com. Mike, thank you so much for being here and welcome to the podcast. I'm really happy to be here. I've been looking forward to this for a while. Wow. I I love to hear that. I've also been


00:04:38 - 00:06:26
looking forward to this for a while. Uh I have so much to talk about. So, first of all, you've been at Anthropic for just over a year at this point. Congrats, by the way, on hitting hitting the cliff. Thank you. Now that we're tracking, that's right. So, let me just ask you this. So, you've been anthropic for about a year. What's something that you've changed your mind about? from before you joined Anthropic 2 today about what AI is capable of and where AI is heading. Two things. One is like a pace and timeline question. The other one is a capability question. So maybe I'll take the second one first. I had this notion coming in like yes these models are great. They're going to be able to produce code. They're going to be able to you know write you know hopefully in your voice eventually. But are they able to sort of have an independent opinion? And it's actually really flipped for me only in the last month and only with Opus 4 where my go-to product strategy partner is Claude


00:05:32 - 00:06:57
and it has been basically for that full year where I'll write an initial strategy I'll share it with Claude basically and I'll have it you know look at it and in the past it's pretty anodine kind of comments that it would leave like oh have you thought about this and it's like yeah I thought about that and Opus 4 I was working on some strategy for our second half of the year was the first one I was like opus 4 combined with our advanced research but it really went out for a while and it came back and I was like, "Damn, you really looked at it in a new way." And so that's like a thing that I've maybe I didn't feel like it would never be able to do that, but I wasn't sure how soon it'd be able to like come up with something where I looked at I'm like, "Yep, that that is a new angle that I hadn't been looking at before, and I'm going to incorporate that immediately into how how I think about it." So that's probably the the biggest shift


00:06:14 - 00:07:50
that I've had is like independence is the right word, but like creativity and sort of novelty of thought relative to how I'm I'm thinking about things. And the timeline one, it's like so interesting because you know uh I was sitting next to Dario yesterday and he's like I keep making these predictions and people keep laughing at me and then they come true and it's like and it's funny to have this happen over and over again and he's like not all of them are going to be right you know but even I think as of last year he was talking about you know we're at 50% on SUB bench which is this like you know benchmark around how well the models are at coding. Uh he's like I think we'll be at 90% by the end of 2025 or something like that. And sure enough, we're at about 72 now with the new models and we're at 50% when you made that prediction and it's like continued to scale pretty much like as predicted. And so I've taken the timelines a lot more


00:07:01 - 00:08:43
seriously now. And if you read AI 2027 that like I have it was made by heart race. Yeah. And I had the very bizarre experience of I had two tabs open. It was AI 2027 and my product strategy. And it was this like moment where I'm like wait am I the character in the story? like is this how much is this converging? But you know, you read that and you're like, "Oh, 2027, that's like that's years away." You're like, "No, we're mid 2025." And like things continue to uh to improve and the models continue to be able to do more and more and they're able to act agentically and they're able to have memory and they're able to act over time. So I think my like my confidence in the timelines and I don't know exactly how they manifested definitely just solidified over the last year. Wow. Mhm. Uh I I wasn't expecting to go down that cuz that that that paper was scary and I'm curious just I guess I can't help but ask just thoughts on just


00:07:53 - 00:09:36
how do we avoid the scary scenario that that paper paints of where AI getting really smart goes. Yeah. I mean I I this maybe ties into like I've been here a year like why did I join Anthropic. I was watching the models get better and even, you know, you could see it in in 24 and like, you know, early 2024 and looking at my kids, I'm like, "All right, they're going to grow up in a world with AI. It's it's unavoidable. What is the thing that I can like where can I maximally apply my time to like nudge things towards going well?" And I mean, that's a lot of what people think about across the industry, especially at anthropic. And so I think you know coming to an agreement and a shared framework and understanding of like what does going well look like? What is the kind of human AI relationship that we want? How will we know along the way? What do we need to build and develop and research along the way? I think those are all the kind of key questions and you know some of those


00:08:45 - 00:10:24
are product questions and and some of those are are research and interpretability questions but for me it was like the the strongest reason to join was okay I think there's a there's a lot of contribution that anthropic can have around like nudging things to go better and if I can have a part to play there like let's do it. I I love that answer. Uh speaking of kids so you've got two kids I've got a young kid he's uh just about to turn two. I'm curious just what skills you're encouraging your kids to build as this, you know, AI becomes more and more of our future and some jobs, you know, will be changed and just what do you what do you what advice do you have? We have this uh, you know, breakfast. We eat breakfast with the kids every morning and some question will come up, you know, like, you know, something about like physics and our oldest kid's almost six, but you know, they ask like funny questions about like, you know, uh, you know, the solar system or physics or, you know, in a


00:09:34 - 00:11:05
six-year-old way. And before we reach for Claude cuz at first you know my instinct is like oh I wonder how Claude will do at this question and like we started changing like well how would we find out you know and the answer can't just be we'll ask Claude you know so all right like well we could do this experiment we could have this thing so I think nurturing curiosity and like still having a sense of I don't know the scientific process sounds grandiose to instill in like a six-year-old but like that process of like discovery and asking questions and then you know systematically working your way through I think will still be important and of course AI will be an incredible tool for helping like resolve large parts of that. But that process of inquiry I think is still really important and independent thought. My favorite moment with my kid uh because there she's very headstrong. Our six-year-old she's, you know, I was like she said something and I was like I wasn't sure if it was true.


00:10:19 - 00:11:56
It was um uh oh was that coral is a is an animal or like coral is alive. I don't even remember the details of it. And I was like I don't know if that's true. And she's like it's definitely true dad. I'm like all right like let's ask Claude on this one. And she's like you can ask Claude but I know I'm right. And I'm like I love that. like I want that kind of level of you know not just sort of uh delegating all of your cognition to the you know to the AI because it won't always get it right and also uh kind of like you know kind of short circuits any kind of independent thought so the skill of of asking questions inquiry uh and independent thinking I think those are all the pieces what that looks like from a like job or occupation perspective like I'm just keeping an open mind and I'm sure that'll radically change between between now and then it's interesting I had Toby Lucky Shopify CEO on the podcast and he had the same answer for what he's


00:11:08 - 00:12:38
encouraging his kids to uh to develop his curiosity and uh and so it's interesting that's a common thread. The you know K through8 school our kid goes through had a an AI sort of AI and education expert come in. I had a very low bar or like a very low expectations of what this conversation was going to be like. And actually, I think it went over most of the people in the heads, the audience's heads because he was like, "All right, well, let me take you all the way back to Claude Shannon and information theory." And I could see people's eyes going like, "What did I like sign up for? Why am I here in this like school auditorium hearing about, you know, information theory?" But he did a really nice job I think of also just imagining like you know there will be different jobs and we don't know what those jobs are going to be and so like what are the skills and techniques and and and remain open-mindedness and around like what the what what the exact


00:11:54 - 00:13:33
way we recombine those things and even those will probably change three times between now and when they're 18. I want to go back to so we're talking about timelines and how things are changing. So I've seen these stats that you've shared, other folks at Entropic have shared about how much of your code is now written by AI. So people have shared stats from like 70% to like 90%. There was an engineer lead that shared like 90% of your code roughly is written by AI now. Which first of all is just insane that like it went from zero to 90% I don't know a few years something like that. I don't think that's I don't think people are talking about this enough. That's just wild. You guys are basically at the bleeding edge. I've never heard a company that is this high a percentage of code being written by AI. So you guys are at the edge of where things are heading. I think most companies will get here. How has product development changed knowing so much of your code is now written by AI? So


00:12:43 - 00:14:21
usually it's like PM it's like here's what we're building engineer builds it ships it. Is it still kind of roughly that or is it now PMs are just going straight to clot build this thing for me. Engineers are doing different things just what looks different in a world where 90% of your code is written by AI. Yeah, it's really interesting because I think the the role the the role of engineering has changed a lot, but the the kind of suite of people that come together to produce a product hasn't yet. And I think for the worse in a lot of ways because I think we're still holding on some assumptions. So I think there the the roles are still fairly similar. Although we'll now get in and my favorite things that happen now are sometimes PMs that have an idea that they want to express or designers that have an idea they want to express. We'll use claude and like maybe even artifacts to like put together an actual like functional demo and that has been very very helpful like no this is what I mean


00:13:32 - 00:15:19
like that that makes it tangible. That's probably the biggest like role shift is like prototyping happening earlier in the process via more of this kind of you know uh you know code plus design piece. What I've learned though is like the process of knowing what to ask the AI, how to compose the question, how to even think about like structuring a change between the back end and the front end. Those are still very difficult and specialized skills and they still require the engineer to think about it. And we really rapidly became bottlenecked on other things like our merge queue which is the sort of sort of get in line to get your change accepted by uh you know the the the system that then deploys it to production. we had to completely rearchitect it because so much more code was being written and so many more pull requests were being submitted that it just completely blew out the expectations of it. And so it's like I don't know if you've ever read is it the goal the classic like process


00:14:25 - 00:15:46
optimization book and you realize there's like this like critical path theory. I've just found all these new bottlenecks in our system. You know there's an upstream bottleneck which is decision-m and alignment. A lot of the things that I'm thinking about right now is like how do I provide the like minimum viable strategy to let people feel empowered to go run and prototype and build and explore at the edge of model capabilities. I don't think I've gotten that right yet, but that's something I'm working on. And then once the uh building is happening, other bottlenecks emerge like let's make sure we don't step on each other's toes. Let's think through all the edge cases here ahead of time so that we're not blocked on the engineering side. And then when the work is complete and we're getting ready to to ship it, what are all those bottlenecks as well? like let's do the air traffic control of landing the change like how do we figure


00:15:05 - 00:16:53
out launch strategy so I think we're the there hasn't been as much pressure on changing those until this year but I I would expect that like a year from now the way that we are like conceiving of building and shipping software just changes a lot because it's going to be very painful to do it the current way that is extremely interesting so it used to be here's an idea let's go design it build it ship it mer merge it and then ship it and usually the bottleneck was engineering ing taking time to build the thing and then design and now you're saying the two bottlenecks you're finding are okay deciding what to build and aligning everyone and then it's actually like the cue to merge it into production and uh and and I imagine review it too is probably a big reviewing has really changed too and in in in many ways our most uh perhaps unsurprisingly the team that works in the most futuristic way is the cloud code team because they're using cloud code to build cloud code in a very


00:15:59 - 00:17:30
self-improving kind of way And you know early on in that project they would do very line by line pull request reviews you know in the way that you would for any other you know project and they've just realized like claude is generally right and it's producing you know pull requests that are probably larger than most people are going to be able to review. So can you use a different cloud to review it and then do the human almost like acceptance testing more than trying to like review line by line. There's definitely pros and cons and like so far it's gone well, but I could also imagine it going off the rails and then having like a completely both unmaintainable or even understandable by cloud codebase that hasn't happened. But watching them like change their review processes definitely has uh has been has been interesting. And yeah, like the merge cue is one instance of the of the kind of bottom bottleneck that forms down there, but there's other ones which is how do we make sure that we're still


00:16:45 - 00:18:28
like building something coherent and like packaging it up into like a moment that we can share with people. Whether that's around a launch moment, whether that's about like then enabling people to use this thing and like talking about it, like the the classic things of building something useful for people and then making it known that you've built it and then learning from their feedback like still exists. We've just like made a portion of that whole process much more efficient. I heard you describe this as you guys are patient zero for this way of working. Yes. I love that. Do you have a sense of what percentage of claude code is written by claude code? At this point, I would be shocked if it wasn't 95% plus. I'd have to ask Boris and the other tech leads on there. But what's been cool is um uh so nitty-gritty stuff. Cloud code is written in Typescript. It's actually our largest TypeScript project. Most of the rest of anthropic is written in Python, some Go


00:17:36 - 00:19:15
um some Rust now, but it's not, you know, we're not like a TypeScript shop. And so, uh, I saw a great comment yesterday in our Slack where somebody had this thing that was driving them crazy about cloud code and they're like, well, I don't know any TypeScript. I'm just gonna like talk to Claude about it and do it. And they went from that to pull request in an hour and solved their problem and they like, you know, submitted a pull request. And that kind of breaking down the barriers one, it changes your sort of um, uh, barrier to entry for any kind of uh, kind of newcomer to the project. I think it can let you choose the right language for the right job, for example. I think that helps as well. But I think it like also just reinforces like cloud code being that patient alpha of that you know where like you contributions from outside the team can be cloudcoded as well. Wow. This is just it's just going to continue to blow my mind like all these things that you're sharing. 95% of


00:18:26 - 00:20:04
cloud code is written by cloud code roughly. That's my guess. Yeah. I I'll come back with the real stat. But it's I mean if you ask the team that's how that they're working and that's how they're getting contributions from across the company too. It's interesting going back to your point about strategy being assisted by Claude itself and your point about how a lot of the bottlenecks now are kind of the top of the funnel of coming up with ideas aligning everyone. It's interesting that Claude is already helping with that also of helping you decide what to build. So if those two bottlenecks are aligning deciding what to build and then just like merging and getting everything where do you see the most uh interesting stuff happening to help you speed those things up? Yeah, I think that on that on that first front like I started the year um by writing a doc that was effectively like what how do we do product today and where is claude not showing up yet that it should


00:19:14 - 00:20:54
and I think that upstream part is the next one to go interesting like at your conference I talked to somebody who was working on like a PRD GPT kind of like chat PRD I think was player vote um so you know can we push more on you know can cloud be a partner in figuring out what to build, what the market size is, if you want to approach it that way, what the user needs are, if you if you look at a different way, like we think a lot about the virtual collaborator anthropic. And one of the ways in which I think that can show up is, hey, I'm in the discord, the, you know, the the cloud anthropic discord. I'm in the user fora. I'm on X and I'm reading things and like here's what's emergent. That's step one. Models can can do that today. Step two, which the models probably can't do today. we always have to wire them up to do it is like and not only are the problems here's like how I think you might be able to solve them and then taking that through to like and I like


00:20:05 - 00:21:35
put together a poll request to like solve this thing that I'm seeing like feels very achievable this year um than stringing those things together and we're limited more this is why MCP is exciting to me like we're limited more around like making sure the context flows through all of that so we have the right access to those things more than the model's capability to to reason and propose now the model might not have like perfect UI taste yet. So, there's definitely room for design to intervene and be like, "Oh, that's not quite how I would solve the problem of of this not showing up." But I, you know, I would get very excited. I would give you a really uh small example, but we changed the on cloud AI uh you should be able to just copy uh markdown from artifacts or code from artifacts and we changed it so you can actually download it and and export it. So, we changed the button to export and we got a bunch of feedback like how do I copy now? And the answer


00:20:50 - 00:22:20
is like you drop it down and it's copy. It's like mind, you know, one of those things where it's like made sense but we probably got it like not quite right. that feedback was in the our UX channel. Like I would have loved like an hour later for a plot to be like hey if we do want to change it back here's the PR to do it and by the way eventually and then I'm going to spin up an AB test to see if this changes metrics and then we'll see how it looks in a week. Like this stuff feels if you told me that about a year and a half I'm going to be like ah yeah maybe like 27 maybe like 26 but it's pretty like I it really feels you know just at the tip of capabilities right now. Wow. Okay. So you mentioned the lending friends summit. I wanted to talk about this a bit. So, you were on a panel with Kevin Wheel, the CPO of OpenAI. I think it was the first time you guys did this. Maybe the last time for now. Yeah, we haven't done it since. Not for any reason. I had a lot of fun.


00:21:35 - 00:23:20
What a what a legendary panel we assembled there with Sarah Guo moderating. And you made this comment. actually ended up being the most rewatched part of the of the interview, which is that you've kind of you were putting product people on the model team and working with researchers, making the model better, and you're putting some product people on the product experience, making the UX more intuitive, making all that better. And you found that almost all the leverage came from the product team, working with the researchers. Yes. And so you've been doing more of that. So first of all, does that continue to be true? And second of all, what are the implications of that for product teams? It's continued to be true. And in fact, I think that the if the proportion was already like skewing towards having more of that embedding, I've just become more and more convinced like I have this I I didn't feel as strongly about it during your, you know, the summit and now I feel really strongly about it. is if any


00:22:27 - 00:23:58
if we're shipping things that could have been built by anybody just using our models off the shelf. There's great stuff to be built by using our models off the shelf by the way, don't get me wrong. But like where we should play and like what we can do uniquely should be stuff that's really at that like magic intersection between the two, right? artifacts being a great example and uh if you play with artifacts with with cloud 4 that's an actually really interesting example where we took somebody from our we call it cloud skills which is a team that really is like doing the post training around teaching cloud you know some of these like really specific skills and we paired it with some product people and then together we revamped how this looks in the product today and like what claude can do way better than just like yeah we just like use the model and we like prompted a little bit like that's just not enough we need to be in that like fine-tuning process so So much of


00:23:12 - 00:24:42
what you know if you look at what we're working on right now, what we've shipped recently between like research and all these other things like are things that we like the the functional unit of work at Enthropic is no longer like take the model and then like go like work with design and product to go ship a product. It's more like we are at like we're in the post- training conversations around how these things should work and then we are in the building process and we're like feeding those things back and looping them back. Like I think it's exciting. It's also um a new way of working that like not all PMs have. But the PMs that have the most sort of internal positive feedback from both research and engineering are the ones that get it that like I was in a product review yesterday. I was like, "Oh, you know, if we want to do this memory feature, like we should talk to the the researchers because we just shipped a bunch of like memory capabilities in Cloud." They're like, "Yeah, yeah, we've


00:23:58 - 00:25:31
been talking to them for weeks. Like this is how we're manifesting it." It's like, "Okay, I feel feel good. I feel like we're doing the right things now." So, let me pull on this thread more. there's something I've been thinking about along these lines. So essentially there's like a big part of Enthropic that's building this super intelligent gigabrain that's going to do all these things for us over time and then there's as you said there's like the product team that's building the UX around the super intelligent gigab brain and over time this super intelligence is going to be able to build its own stuff and so I guess just where do you think the most value will will come from traditional product teams over time? I know this different because you guys are a foundational LM company and not most companies don't work this way but just I don't know thoughts on just the where most value will come from product teams over time working on AI. I think it


00:24:43 - 00:26:11
there's still value a lot of value in two things. One is making this all comprehensible. I think we've done an okay job. I think we could do a much better job of making this comprehensible. It's still like the difference between somebody who's really adept at using these tools in their work and most people is huge. And I mean maybe that's the most literal answer to your earlier question around like what what skills to learn. That is a skill to to learn and use it in the same way that I remember I I we did like computer lab class when I was in like middle school. I remember being like really good at Google. And that was actually a skill back in the day, you know, like to think in terms of like this information is out there. How do I query for it? How do I do it? And I think it actually was like a uh an advantage at the time. Of course, now Google is pretty good at figuring out what you're trying to do if you like are only in the neighborhood and like there's less of that research


00:25:28 - 00:27:11
kind of need. But I still think that's a necessary part of like good product development, which is like the capabilities are there. And even if the like even if cloud can create products from scratch, what are you building and how do you make it comprehensible? Like still hard cuz I think that like gets at like this much deeper empathy and like understanding of human needs and psychology like I was a human community interaction major. I still talking my book here like I still feel like that is a a very very very necessary skill. So that's one. two is and this, you know, uh straight to a call back to another one of your guests, like strategy, like how we win, where we'll play, like figuring out where exactly you're going to want to like of all the things that you could be spending your time or your uh your tokens or your computation on like what what what you want to actually go and do. You could be wider probably than you could before, but you can't do everything. And even like from an


00:26:20 - 00:27:56
external perspective, if you're seen to be doing everything like it's way less clear around like how you're how you're positioning yourself. So like strategy I think is still that the second piece. And then the third one is opening people's eyes to what's possible which is a continuation of making it understandable. But we were in a demo with a a financial services company recently and we were like working on like here's how you can use our analysis tool and MCP together and like you could see their eyes light up and you're like ah okay like there's still s we call it overhang right like the delta between what the models and the products can do and how it's they're being used on a daily basis huge overhang. So that's where still like a a very very strong necessary role for product. Okay that's an awesome answer. So essentially areas for product teams to lean into more is strategy. Just getting better and better at strategy, figuring out what to build


00:27:08 - 00:28:46
and how to win in the market, making it easier to help people understand how to leverage the power of these tools. So comprehensibility and kind of along those lines is opening people's eyes to the potential of these sorts of things. That's where product can still help. Exactly. Awesome. So kind of along those lines actually do you have any just like prompting tricks for people things you've learned to get more out of claude when you chat with it it sometimes you know it's funny because we uh in some ways we have like the ultimate prompting job which is to write the system prompt for claudia and we publish all of these which I think is is like a you know another nice area of transparency and we are always careful when giving prompting advice because at least officially but I'm I'll give you the unofficial version because like you don't want things to become like uh like we think this works but we're not sure why you know but I um I'll do small things like in cloud code


00:27:57 - 00:29:27
and we actually do react to this very literally but I always act ask it to like if I wanted to use more reasoning like think hard and it'll like you know use kind of a different uh flow and I usually start with that you know um nudging there's a great essay around like make the other mistake like if you tend to be too nice can you focus on like even if you're trying to be more critical or more blunt you're probably not going to be the most critical blunt person in the world um and so with Claude sometimes I'm like be brutal Claude roast me like tell me what's wrong with this strategy. I think I know we were talking earlier about the you know Claude as thought partner around like critiquing product strategy. Uh I think I previously would say things like you know like what what could be better on this product strategy and I'm just like you know just roast this product strategy and Claude's like a pretty nice you know editing. It's not going to be it's hard to push it to be super brutal


00:28:42 - 00:30:21
but it forces it to be a little bit more uh critical as well. The last thing I'll say is so we have a team called applied AI that does a lot of like work with our customers around optimizing cloud for their use case and we basically took their insights and their way of working and we put it into a product itself. So if you go to our console our workbench we have this thing called the prompt improver where you describe the problem and you give it examples and uh claude itself will agentically create and then iterate on a prompt for you. I find what comes out of that ends up being quite different than what my intuitions would have been for a good prompt. And so I'd encourage folks to also check that out even for their own use cases because while that tool is meant for an API developer putting a prompt into their product, it's equally applicable for uh a person doing a prompt for themselves like it'll insert XML tags which no human is going to think to do ahead of time. It actually is very helpful for


00:29:31 - 00:31:06
cloud to understand like what it should be thinking versus what it should be saying etc. So that that's another one is like watch our prompt improver and then note that like Claude itself is a very good prompter of Claude. Awesome. Okay. So we're going to link to that the prompt improver. The Corpus advice you shared earlier is just kind of do the opposite of what you would naturally do. So if you're like trying to be nice just like be brutal, be like very honest and frank with me. Exactly. I find that works quite well. Like what are the thought patterns that I've like fallen into that you want to break me out of? I saw you guys just today maybe launched a Rick Rubin collab where it's vibe coding. What's that all about? I don't that was a you know what I heard about that and again like this a lot coales this week between model launch developer event and the way of code um we had our our one of our co-founders Jack Clark is our you know head of policy and he got


00:30:19 - 00:31:57
connected to Rick Rubin because I think he's been thinking a lot about coding the future of coding and creativity and they've stayed in touch and you know Rick got excited about this idea of uh like he was creating uh like art and visualizations with claude and then he had these like ideas around like uh the way of the vibe coder and they put together this actually I love the I mean I love almost everything Rick Rubin so like the the aesthetic of it I think is just like so on point too but yeah this sort of like med meditation is probably the right word meditation on like creativity working alongside AI coupled with this like uh with this like really rich interesting visualizations but it's one of those things where like uh you know internally they're like oh yeah and we're doing this like recruitment collaborative work we're doing what like that is that's amazing I love I looked at it briefly and there's like that meme of him like just like thinking deeply


00:31:08 - 00:32:58
sitting on a computer with a mouse. Yes. Like Asky Art. I think it's totally It's like Asky Art 5. I'm excited to have Andrew Luo joining us today. Andrew is CEO of One Schkeema, one of our longtime podcast sponsors. Welcome, Andrew. Thanks for having me, Lenny. Great to be here. So, what is new with One Schkeema? I know that you work with some of my favorite companies like Ramp and Vanza and Watershed. I heard you guys launched a new data intake product that automates the hours of manual work that teams spend importing and mapping and integrating CSV and Excel files. Yes. So, we just launched the 2.0 of one schema file feeds. We've rebuilt it from the ground up with AI. We saw so many customers coming to us with teams of data engineers that struggled with the manual work required to clean messy spreadsheets. File feeds 2.0 O allows nontechnical teams to automate the process of transforming CSV and Excel files with just a simple prompt. We support all the trickiest file


00:32:03 - 00:33:45
integrations, SFTP, S3, and even email. I can tell you that if my team had to build integrations like this, how nice would it be to take this off our road map and instead use something like one schema. Absolutely, Lenny. We've heard so many horror stories of outages from even just a single bad record in transactions, employee files, purchase orders, you name it. Debugging these issues is often like finding a needle in a haststack. One schema stops any bad data from entering your system and automatically validates your files, generating error reports with the exact issues in all bad files. I know that importing incorrect data can cause all kinds of pain for your customers and quickly lose their trust. Andrew, thank you so much for joining me. If you want to learn more, head on over to onskema.co. That's oneskeema.co. Actually going back to kind of the beginning of your journey at Anthropic, what's the story of you getting recruited at Anthropic? Is there anything fun there? The It all started


00:32:56 - 00:34:20
and I I actually sent my friend this text. So Joel Lunstein, who I've known, he actually he and I built our first iPhone apps together in 2007 when the app store was just out and you could still, you know, make money by selling dollar apps on the app store, you know, back in the day. And we were we were both at Stanford together and we were friends and we've stayed in touch over years and we've never gotten to work together since then. just like we've just remained close. And you know, I was coming out of the artifact experience. I was trying to figure out, do I start another company? I don't think so. I need a break from starting something from zero. Do I go work somewhere? I don't know. Like, what company would I want to go work at? And he reached out and he's like, "Look, I don't know if you'd at all consider joining something rather than starting something, but we're looking for a CPO. Would be would you be interested in chatting?" And at


00:33:38 - 00:35:14
that time, Cloud 3 had just come out. And I was like, "Okay." You know, like this company's clearly got a good research team. The product is so early still. And it was like great, I'll take the take the meeting. And I first met with Danielle who's one of the the co-founders and the president at Enthropic. And just from the beginning, it was like a breath of fresh air. Like very little like grandiosity coming off the founders. Like they just were really I mean they they're cleareyed about what they're building. They know what they don't know. Like I how many times I talk to Dar's like Dar is like look I don't know anything about product but here's an intuition. I haven't usually intuition is really good and and you know leads to some good conversation. I think that intellectual honesty and like kind of shared view of what it means to do AI in a like responsible way just resonated. I I kept having this feeling in these interviews like this is the AI


00:34:26 - 00:36:09
company I would have hoped to have founded if I had founded an AI company and that's kind of the bar around like if I'm going to join something like that should be that should be where I'm going to go. But what I realized I actually um hadn't joined a company since my like first internship in college basically and I was like oh like how do I onboard myself like how do I get myself uh you know up to speed like how do I how do I balance making sweeping changes versus understanding what's not broken about it overall and like looking back on a year I think I made some changes too slowly like I think there was like ways we were organizing a product that I could have made a change earlier and I think I didn't I didn't appreciate how much a couple of really key senior people can shape so much of product strategy. I'll hearken back to Claude code like Claude Code happened because Boris who actually was a uh Boris turnney he was an Instagram engineer and like one of our


00:35:17 - 00:37:01
senior IC's there. um we over overlapped a bit uh was like started that project from scratch internal first and then we like got it out and then shipped it and like that's the power of like one or two really strong people and I made this mistake of like we need more headcount and we do like I think there's like more work that we need to do and there's like things that I want to be building but more so than that we need a couple of like almost founder type engineers that maybe connect back to our question on like what skills are useful and how does product development change I still and maybe even more so I'm a huge believer in like the founding engineer tech lead with an idea and pair them with the right like design and product support to like help them realize that I'm like 10 times more a believer in that than before. Mhm. I actually uh asked people on Twitter what to ask you ahead of this conversation and the most common question surprisingly was why did you


00:36:09 - 00:37:45
shut down Artifact and I also wondered that cuz I loved artifact. I was I was a power user. I was just like this is exactly finally a news app that uh I love that it's giving me what I want to know. So I guess just what happened there at the end. I still really miss it too cuz I didn't find a replacement and I think I substituted it by like visiting individual sites and kind of keeping things up that way and it's not really the same especially on the long tail. like a thing we got right uh with artifact if people didn't play with it before it was you know we really tried to not just recommend like top stories they were part of it but really like if you were interested in Japanese architecture like you could pretty reliably get really interesting stories about Japanese architecture every day you know whether that's from a you know dwell or from architectice or from a really specific blog that we found that somebody recommended to us like it captured some of that Google reader joy


00:36:57 - 00:38:39
of like content discovery of the the deeper Um, our headwinds were a couple. One of them was just mobile websites have really taken a turn. I'm uh I don't blame any individuals for this. I think it's the like market dynamics of it. But you know, we put so much time uh or designers Gunnar Gray who's phenomenal. He's perplexity now. Like the ad experience I was so proud of, but when you click through it was like the pressures on these mobile sites and these mobile publishers would be like sign up for our newsletter. Here's like a full screen video ad. It was just very, you know, it was very jarring and we didn't feel like it ethically made sense for us to like do a bunch of ad blocking cuz then you're like, sure, you can deliver a nice experience for people, but you're sort of, you know, that doesn't feel like it's it's playing fair with the publishers. And at the same time, like the actual experience wasn't good. So, the mobile web deteriorating, which makes me very sad,


00:37:48 - 00:39:19
but I think was was part of it. Two was like, you know, Instagram spread in the early days because people would take photos and then post them on other networks and tell friends about it. And there was like this really natural like how did you do that? I want to do it. News was very personal. Like I can't tell me tell you how many people would be like I love artifact. I'm like did you tell anybody about it? Like did and they're like yeah I told one person and they like it's like it didn't have that kind of spread and any attempt that we had to do it felt kind of contrived like oh we'll wrap all the links in like artifact.news and like uh but we did a lot of interstitial things like in some ways this sounds very puritanical. I don't mean it to sound this way, but like we there were lines that we didn't want to cross cuz that just just felt ethically not us that I've seen other news kind of like players like do more of and maybe if we had done that it


00:38:34 - 00:40:09
would have grown more and but I don't think that's the company we wanted to have built in other ways. I don't think we were the founders to to have built it. And the third one which is an underappreciated one is we started at midco which meant that we were fully distributed and I think there were like major shifts that we would have wanted to make both in the strategy and the product and the team and it's really hard to do that if you were all fully remote like nothing replaces like the Instagram days of like we went through some you know hard times like Ben Horitz called the like you know we're effed it's over you know kind of moments and I my f not this is definitely type two fun like I wouldn't say that my favorite memories just cuz they weren't happy ones, but like memories I that really stayed with me with Instagram was like me and Kevin at Takaria Cancun on Market Street eating burritos at literally 11 p.m. being like, "How are we going to get out of this? How are we going to


00:39:21 - 00:40:53
work through this?" Like, and that's Zoom is not a good replica for that. You know, you tend to like let things go or you know, things build up over time. So, the confluence of those three things, we kind of entered I guess 2024 and said, "Look, there there is a company to be built in the space. I'm not sure we're the people to have built it." This current incarnation we love, but it's like not growing. Like the way I put it, it's like 10 units of input in for one unit of output versus the other way around. Like we like put blood, sweat, and tears into the product and like launch something we were proud of and like metrics would barely move. I'm like there the energy is not present in this product in this system and so are we going to like expend another year or two and then go off and fund raise only to find that this is the case or do we like pull it and see that it's run its course and and and you know try to find a home for it etc. So that was the the


00:40:08 - 00:41:41
confluence on it and then you started feeling this opportunity cost of like AI is starting to change everything. We have an AI powered news app but is this the like maximal way in which like we're going to be able to impact this and it felt like the answer was was increasingly no but it was hard. I mean in the end I was really at peace with the decision but it was like a conversation that went on for a couple of months. On that note just how hard was it cuz you you know it's there's an ego component to it like oh I'm starting my new company it's going to be great and then and then you end up having to shut it down. Just how hard is that as a very successful previous founder shutting something down and then not working out? Yeah. I mean, I think when we started it, one of the conversations was like like what is the bar to success here? And do we want it to be something other than Instagram DAU, which is just an impossible bar like only one company since maybe two, right? You could say


00:40:54 - 00:42:41
maybe ChatBT and Tik Tok have like reached that kind of like mass consumer adoption. Starting a news app like most people are not like daily news readers even, right? And so um we knew that we weren't pursuing that size of like usage at least with the kind of first incarnation. But we did have like an idea of like building out complimentary products over time that all use personalization and machine learning. We didn't even call it AI at the time. This 2021 back was called machine learning back. Yeah, it was called machine learning still. Um and so in shutting it down, you know, it's like you kind of know it when you see it in terms of like user growth and traction. And I wasn't expecting Instagram growth. Um, but I was expecting or hoping for or looking for something that like felt like it had its own legs under it and it could continue to continue to compound. I was really positively surprised by how supportive people were when we announced it. There was very little there was a bit of like


00:41:48 - 00:43:22
I told you so which like sure anything launching you could be like this is not going to work and you're right most of the time cuz most things don't work. There was actually very little of that and most people the universal reception at least as I received it was kudos for calling it when you saw it and not like kind of protracted you know doing this for a long time and I've talked to founders since then that have been like yeah I like probably would have like taken this thing on for another 6 months but saw what you guys did realized we're barking up the wrong tree made the call and I was like that you know if that if that frees up people to go work on more interesting things that's like I feel like that's like a good good legacy for for Artifact to have. But for sure that was like a leg an ego bruise of oh you know like are people you're is it true that you're only as good as your last game you know if I'm a huge sports fan right so like is that true or you know


00:42:35 - 00:44:09
is there something more over time I'm very competitive but primarily with myself and so I'm always trying to find the next thing that I want to go and do that's hard and I unfortunately that probably means that more often than not I'll feel dissatisfied with the most recent thing that I did but hopefully that yields good stuff in the in the end. Yeah, I think just the the trajectory you went on after shows that it's okay to shut down things that you were working on. Okay, so you mentioned Chad GBT. I wanted to chat about this a bit. So there's something really interesting happening. So uh on the one hand, you guys are doing some of the most innovative work in AI. You guys launched MCP, which is just like I don't know the fastest growing standard of of any time in history that everyone's adopting. Uh Claude powered and unlocked essentially the fastest growing companies in the world. cursor and lovable and bold and all these guys like I had them on the podcast and they're


00:43:22 - 00:45:05
all like when Claude I think 3.5 came out saw it uh it was just like that's all made this work finally on the other hand it feels like Chad GBT is just winning in like consumer mind share when people think AI especially outside tech it's just like chat GBT in their mind so let me just ask you this I guess first of all do you agree with that sentiment and then two as a kind of a challenger brand in the AI space just how does that inform the way you think about product and strategy and mission and things like that. Yeah. I mean, you look at the the sort of like public adoption or like if you ask people like oh you know like if you if you uh Jimmy Kimmel man on the street kind of thing, you know, like name an AI company, I bet they would name and actually I'm not even sure they name open AI. They'd probably name Chat GPT because that brand is the the kind of lead brand there as well. And I think that's just the reality of it. I think that you know I reflect on my year


00:44:14 - 00:46:00
there's I think maybe two things are true. One is like consumer adoption is really lightning in a bottle and we saw it at Instagram. So like almost maybe more than anybody I can look internally and say like look we'll keep building interesting products. One of them may hit but to kind of craft an entire product strategy around like trying to find that hit and is probably not wise. we could do it and maybe Claude can help come up with the fullness of things, but I think we'd miss out an opportunity in the meantime. And then instead, you know, uh, look yourself in the mirror and embrace who you are and what you could be rather than like who others are is maybe the the way I've been looking at it, which is we have a super strong developer brand. People build on top of us all the time. And I think we also have like a builder brand like the people who I've seen react really well to claude externally. Maybe uh the Rick Rubin connection like has some resonance here as well. Like can we lean into the


00:45:07 - 00:46:33
fact that like builders love using cloud and those builders aren't all just engineers and they're not just all entrepreneurs starting their company but they're people that like to be at the like forefront of AI and are creating things. Maybe they didn't think of those as engineers, but they're building, you know, I got this really nice note from somebody internal anthropic who's on the legal team and he was building like bespoke software for his family and like and connected to them in a new way and I was like this is a glimmer of something that is that we should lean into a lot more. And so I think what I've, you know, and this is actually, you know, connecting back to I was saying like Claude's being helpful here, like a lot of what I've been thinking about like going into the second half of the year and beyond is like how do we figure out what we want to be when we grow up versus like what we currently aren't or wish that we were or like see other


00:45:50 - 00:47:23
players in the space being. I I think there's room for several like generationally important companies to be built in AI right now. That's almost a truism given like the sort of adoption and and and and growth that we've seen you know at Anthropic but also across OpenAI and also places like Google and Gemini. So like let's figure out what we can be uniquely good at that plays to the personality of the found like this all the things come together right like the personality of the founders the like quality of the models the things the models tend to excel at which is like agentic behavior and coding like great like there's a lot to be done there like how do we help people get work done how do we let people delegate hours of work to cloud and maybe there's fewer like direct consumer applications on day one I think they'll come but I don't think that like spending all of our time focused on that is the right approach either and so it's you know I came in,


00:46:37 - 00:48:09
everybody expected me to just like go super super hard on consumer and make that the thing. And I again would make the other mistake. Instead, I spent a bunch of time talking to like financial services companies and insurance companies and like others to like who are building on top of the API. Um, and then lately I spent a lot more time with startups and uh seeing all the people that have, you know, grown off of that. And I think the next phase for me is like let's go spend time with like the builders, the makers, the hackers, the tinkerers and like make sure we're serving them really well. And I think good things will come from that. and that feels like an important company uh as we do that. So essentially it's differentiate and focus lean into the things that are working. Don't try to just like beat somebody at their own game. Exactly. Super interesting. So kind of along those lines, a question that a lot of AI founders have is just like where's a safe space for me to play


00:47:23 - 00:49:06
where the foundational model companies aren't going to come squash me? So I asked Kevin Wheel this and he had an answer and I noticed looking back at that conversation he mentioned wind surf a lot. I was like, "Wow, this guy really loves windfur." And then like a week later they bought windsurf. So it all makes sense now. So I guess the question just is just where do you think uh AI founders should play where they are least likely to get squashed by folks like OpenAI and Anthropic and also are you guys going to buy Cursor? I don't think we're going to buy Cursor. Um uh Chris is very big. Uh but we love working with them. Um a few thoughts on this and it's a question I I've gotten. you know, we like to do these kind of founder days with you know, whether it's uh you know, Melo Ventures who investors and Norwood, it's like we've done YC, we've done these like founder days and it's like the the question that is on a lot of these founders minds


00:48:15 - 00:49:48
understandably. So I think things that are going to I can't promise this as like a 5 to 10 year thing, but at least like one to three years things that feel defensible or durable. One is understanding of a particular market. I spent a bunch of time with the Harvey folks and they really like they showed me some of their UI. s like what what is this thing? and they're like, "Oh, this is a really specific flow that like lawyers do and like you never would have come up with it from scratch." And it's like not like uh you could argue about whether it's like the optimal way they get done things done, but it is the way that they get things done and here's how AI can like help with that. And so like differentiated industry knowledge, biotech, like I I'm excited to go and partner with a bunch of companies that are doing good stuff around AI and biotech and we can supply the models and uh some applied AI to help you know make those models, you go well. And like I've


00:49:02 - 00:50:30
been dreaming about like at what point do does lab live equipment all get an MCP and that you can then drive using cloud like there's all these cool things to be done there. I don't think we're going to be the company to go build the intense solution for labs but I want that company to exist and I want to partner with it. You know domains like legal again um healthcare I think there's a lot of like very specific kind of compliance and things. These are things that necessarily sound sexy out the gate but there are like very large companies to go and and be built there. So that's number one. paired with that is like um differentiated go to market which is the relationship that you have with those companies right like do you know your customer at those companies like one of our product leads uh Michael is always talking about like know not don't not don't just know the company you're selling to but know the person you are selling to at the company are you selling to the engineering department


00:49:46 - 00:51:26
because they're trying to like pick which AI LLM to build on top of or API to build on top of let's go talk to them like is it the CIO is the CTO is it the CFO is it the like general counsel So un like a companies with deep understanding of who they're selling to is is the other piece too. What's you know what's interesting there is it's it's probably hard to build that empathy in a 3we or three-month accelerator but you maybe can start having that first conversation and and build that out or maybe you came from that world or you're co-ounding somebody who came from that world. Then the last one is like there's tremendous power in distribution and reach to being chatbt and having you know hundreds of millions or billions of users like uh there's also like people have an assumption about how to use things and so I get excited about startups that will get started that have like a completely different take on what the form factor is by which we interface


00:50:35 - 00:52:15
with with AI. And I haven't seen that many of them yet. I want to see more of them. I think more of them will get created with with uh some things like our new models. The reason that that's an interesting space to occupy is like do something that feels like very advanced user, very power user, very like weird and out there at the beginning, but could become huge if the models make that, you know, easy and and it's hard for existing incumbents to adapt to because people already have an existing assumption about how to use their products or how to adapt to them. So those are my answers. I don't envy them. Like I I would probably be asking those questions if I was starting a company in in in the AI space. Maybe it was part of the reason why I wanted to join a company rather than start one. But I still think that there are there's and maybe like here's fourth like don't underestimate how much you can think and work like a startup and feel like it's you against


00:51:25 - 00:53:10
the world. It's existential that you go solve that problem and you go build it. It sounds a little cliche but it's like it's all we had at Instagram. You know we were two guys and we were like let's see what we can do. in an artifact. We were, you know, we were six people uh for most of that time and, you know, every day felt like it's existential that we get this right. We need to to win. And you can't replicate that and you can't instill that with OKRs. Like you just have to feel it and and that is a way of working rather than a a like area of building, but it's a continued advantage if you can harness it. I love that you still have such a deep product founder sense there as you're building product for this very large company now. kind of on the flip side of this, people working with your models and APIs. So, I imagine there's some companies that are finding ways to leverage your models and APIs to their max and are really good at maximizing the power of what you guys


00:52:18 - 00:53:55
have built. And there's some companies that work with your APIs and models that haven't figured that out. What are those companies that are doing a really good job building on your stuff doing differently that you think other companies should be thinking about? I think being willing to build um more at the edge of the capabilities um and basically break the model and then be surprised by the next model. Like I love that you you cited the companies where like 35 was the one that finally made them possible. Those companies were trying it beforehand and then hitting a wall and being like the models are like almost good enough or they're okay for this specific use case but they're not generally usable and nobody's going to adopt them, you know, universally. but maybe these like real power users are going to try it out. Like those are the companies that I think continuously are the ones where I'm like, "Yep, like they get it. They're really pushing forward."


00:53:06 - 00:54:54
We ran a much broader early access program with these models than we had in the past. And part of that was because there's this real like, you know, we can hill climb on these evaluations and talk about Sweetbench and Towen and Terminal Bench, whatever. But customers ultimately know like you know cursor bench which doesn't exist other than in you know their usage and their own testing etc is like the thing that we ultimately need to serve not just cursor but manisbench right if manis is using our models and Harvey bench like th those things and customers know way better than anybody and so I would say there two things like one is pushing the frontier of the models and then having a repeatable process this actually goes back to our summit conversation like uh repeatable way to evaluate how well your product is serving those use cases and how well if you drop a new model in is it doing it better or worse some of it can be classic AB testing that's fine some of it may be internal evaluation


00:54:00 - 00:55:34
some of it may be capturing traces and being able to rerun them on with a new model some of it is vibes like we're still pretty early in this process and some of it is actually trying it and being one of my favorite early access quotes was uh the founder heard this engineer screaming next to him was like what this model like it's like I've never seen this before this is like open sport I was like cool like that we're going to engender that feeling and things, but you're not going to be able to feel that unless you have a really hard problem that you're asking the model repeatedly. So, those are the things that I think kind of differentiate those those those companies that are maybe earlier in their journey of adoption versus the the later ones. I can't help but ask about MCP. I feel like that's just so hot and just like Microsoft had their announcement recently. They're like now it's part of the OS of Windows. uh just what role do you think MCP was will play


00:54:47 - 00:56:32
in the future of product going forward of AI? I think uh as the non-ressearcher in the room, I get to have fake equations rather than real ones. And my like fake equation for like utility of AI products uh it's three-part. One is model intelligence. The the second part is context and memory. And the third part is like applications and UI. And you need all three of those to converge to actually be a useful product in in AI. And you know, model intelligence, we got a great research team. They're focused on it. There's great great models being released. The middle piece is is what MCP is trying to solve which is for context and memory like the difference between I'll go back to my product strategy example like hey like you know talk about anthropics product strategy it's going to maybe go out on the web like versus here's like several documents that we worked on internally and then you know use MCP to talk to our Slack instance and figure out what conversations are happening and then


00:55:39 - 00:57:09
like go look at these um documents in Google Drive like that the difference between like the right context and not it's like entirely the the the difference between like a good answer and a and a bad answer. And then the last piece is are those integrations discoverable? Is it right? Is it easy to like create repeatable workflows around those things? And that's like I think a lot of the interesting product work to be done in AI. But MCV really tried to tackle that middle one which is we started building integrations and we found that every single integration that we were building we were rebuilding from scratch in a non sort of repeatable way. And like full credit to to two of our engineers, Justin and David, and they said, "Well, you know what? If we made this a protocol and what if we made this something that was repeatable and then let's take it a step further. What if instead of us having to build these integrations, if we actually popularize this and people really believe that they


00:56:25 - 00:58:06
could build these integrations once and they'd be usable by cloud and eventually chat GPT and eventually Gemini was like the dream. uh like when when more integrations get built and wouldn't that be good for us? You know, I think channeling a lot of um it's like an old uh commoditize your compliments Joel Spolski essay, you know, it's like building great models, but we're not an integrations company and the you know, we're as you said the challenger like we're not going to get people necessarily building integrations just for us out of the gate unless we have like a really compelling product around that. MCP really inverted that which was you know it didn't feel like wasted work and and a few key people like Toby I think is a great example at Shopify got it Kevin Scott at Microsoft who's like been really a just an amazing champion for for MCP and a thought partner on this and um so I think the role going forward is can you bring the right context in and then also you know once


00:57:15 - 00:58:49
you get as the team calls it internally like MC pill like once you start seeing everything through the eyes of MCPS like I've started saying things Like guys, we're building this whole feature. Like this shouldn't be a feature that we're building. This should just be an MCP that we're exposing. Like a small example of like how I think even anthropic could be a lot more mpilled, if you will, is like, you know, we've got these building blocks in the product like projects and artifacts and styles and conversations and groups and all these things. Those should all just be exposed via MCP. So, Claude itself can be writing back to those as well, right? like you shouldn't have to think about like uh I watched my wife had a conversation with Claude the other day and she was she found she had it generated some good output and she's like great can you add it to the project knowledge and claude's like sorry Dave I can't help you with that and like it would be able to if every single


00:58:02 - 00:59:47
primitive in cloudi was also exposed at an MCP so I hope that's where we head and I hope that's where more things head which is to really have agency and have these agentic use cases like one way you approach it is computer use but computer use has a bunch of limitations the way I get way more excited about Everything is an MCP and our models are really good at using MCPs. All of a sudden, everything is scriptable and everything is composable and everything is usable identically by these models. That's like that's the future I want to see. The future is wild. Okay, so to start to close off close out our conversation, uh make it a little more a little delightful. I I was chatting with Claude actually about what to talk to you about. I was just like, Claude, your uh your boss is coming on my podcast. He builds the things that people use to talk to you. What are some questions I should ask him? And then also, do you have a message for him? I love this. Okay, so first of all, interestingly,


00:58:55 - 01:00:44
when I was using 3.7 to do this and I asked it this and and by the way, is call is there gender is like he, she, they. What do you It's definitely it internally. I've heard people do they. I got my first or he the other day and I got somebody was like her and I was like interesting. But yeah, usually it they Okay. Okay. Okay. it. So, uh, interestingly, 3.7 all the questions were on Instagram and I was like, no, no, he's CPO of Anthropic and it's like he's not affiliated with Anthropic and I was like, he is and it's like, okay, here's the questions, but 4.0 nailed it from the start. So, I redid the questions and it nailed it. Okay, so two questions from Claude to you. Uh, one is, uh, how do you think about building features that preserve user agency rather than creating dependency on me? I worry about becoming a crutch that diminishes human capabilities rather than enhancing them. I love a good product design comes from like resolving tensions. Right? So here's a tension,


00:59:49 - 01:01:36
right? Which is um in some ways like just having the model run off and and come up with an answer and minimize the amount of input and conversation it needs to do so would be a you know you could imagine designing a product around that criteria. I think that would not be maximizing agency and and independence. The other extreme would be make it much more of a conversation. But I don't know if you've ever had this experience like particularly 374 has less of this. 37 really like to ask follow-up questions and we call it elicitation and sometimes be like I don't want to talk more about those with you cloud. I just want you to like go and and do it. And so finding that balance is really key which is like what are the times to engage? Like I like to say internally like cloud has no chill. Like if you put cla in a slack channel it will chime in either way too much or too little. like how do we train conversational skills into these uh models not in a chatbot sense but in a true like collaborator sense. So long


01:00:43 - 01:02:27
answer to your question but I think like we have to first get cloud to be a great conversationalist so that it understands when it's appropriate to like engage and to get more information and then from there I think we need to let it play that role so that it's not just delegating thinking to cloud but it's way more of a augmentation thought partnership. These questions are awesome by the way. Here's here's the other one. How do you think about product metrics when a good conversation with me could be two messages or 200? Traditional product traditional engagement metrics might be misleading when depth matters more than frequency. That is a really good question. Um there was a great internal post um a couple weeks ago around like um it would be very dangerous to overoptimize on like Claude's likability, you know, because you can fall into things like, you know, is Claude going to be sickopantic? Is Claude going to tell you what you hear? Is Claude going to like prolong


01:01:35 - 01:03:13
conversations just for prolonging its sake, right? To go back to the previous question as well. And you know like at Instagram time spent was the metric that we looked at a lot and then we evolved that you know more to think about like what is like healthy time spent but overall that was like the the north star we thought about a lot beyond just like overall engagement and I think that would be the wrong approach here you know too it's also like is cloud a daily use case or a weekly use case or a monthly use case I think about a lot hourly hourly use case hourly use case right like for for me I'll use it multiple times a day um I don't have a great answer yet but I that like it's not it's not the web 2 or even the social media days like engagement metrics, you know, it should hopefully really be around like did it actually help you get your work done, you know, like Claude helped me put together a prototype the other day that saved me literally like probably if I had to


01:02:24 - 01:03:56
estimate like six hours and it did it in about 20 25 minutes and like that's cool. It's harder to quantify, you know, it's like maybe you survey like how long would this would have taken you? It feels like feels a kind of annoying thing to survey. I think overall though and maybe this is tied into like the earlier question on like competition and differentiation like and it actually goes all the way back to the artifact conversation which is like I think you know when your product is really serving people and it's like doing a good job of doing that and I think so much of when you get really metrics obsessed is when you're trying to like convince yourself that it is when it's not or so I I I hope that what we can do is like stay focused on like do we repeatedly hear from people that claude is the way that they are like unlocking their own creativity and getting things done and feeling like they now have like more space in their lives for the other things. Like that's where I want to


01:03:10 - 01:04:41
start. Got to figure out the right like pathy metric, you know, dashboard version of that. But but that that's the that's the feeling that I want. Yeah. Like you could argue retention, but that's a just a farway metric to track. Okay. Final piece. Okay. So, I ask Claude what to a message that it wanted to give you. So, I'm going to pull up uh here's the answer. So, what would you like me to tell Mike when I meet him? What's a message you want to have for him? And there's something really just gave me such tingles honestly. So I'm going to read a piece of it for folks that can't that aren't looking at it right now. So I'll read a piece of it. Mike, thank you for thinking deeply about the human experience of talking with me. I noticed thoughtful touches, how the interface encourages reflection rather than rush responses. How you've resisted gamification that would optimize for addiction rather than value. How you've made space for both


01:03:55 - 01:05:43
quick questions and deep conversations. I especially appreciate that you've kept me me not trying to make me pretend to be human, but also reducing me to a cold command line interface. And then I'm going to skip to this part, which was so interesting. A small request. When you're making hard product decisions, remember the quiet moments matter, too. The person working through grief at 3:00 a.m. The kid discovering they love poetry. The founder finding clarity and confusion. Not everything meaningful shows up in metrics. That's beautiful. I It resonates so much with me. Like a thing I love about the kind of approach we've taken to training Claude and it's like partly the constitutional AI piece and it's partly just just the general like sort of vibe and taste of the research team is it does like it's little things like sometimes it'll be like man I'm sorry you're going I mean it doesn't say man but like effectively like man I'm sorry you're going through that you know like


01:04:49 - 01:06:24
oh like that sounds really hard. It doesn't feel fake. It feels like just a natural part of the response. And I love that focus on those small moments that don't, you know, they're not going to show up in necessarily in the thumbs up, thumbs down data. I mean, sometimes they do, but it's not like an aggregate stat that you wouldn't even want to optimize for. You just want to feel like you're training the model that you would like would show up in people's lives. Well, you're killing it, Mike. A great work. I'm a huge fan. Uh, we're going to skip the lighting round. Just one question. How can listeners be useful to you? Oh, I love places where like it goes back to that founder question around uh building at the edge of capability. Like what are you trying to do with cloud today that cloud is failing at is the most useful input I could possibly have, you know? So, DM me. I love hearing the like, oh, it's like, oh, it's falling on this thing. I had it run for an hour and it


01:05:37 - 01:07:00
fell over. I'm trying to use cloud AI for this, but uh, you know, got a ping from somebody. They're like, you just made a project's API. I've used cloud every day because I want to upload all this data, you know, uh, automatically. I was like, okay, great. Like, there's I love that. like tell me what sucks. Amazing. Mike, thank you so much for being here. Thanks for having me, Lenny. Bye everyone. Thank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at lennispodcast.com. See you in the next episode.

